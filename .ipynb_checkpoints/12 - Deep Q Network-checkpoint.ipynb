{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12 - Deep Q Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym.envs.registration:Making new env: CartPole-v0\n",
      "[2017-05-06 16:10:30,069] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.envs.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 128)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        self.fc1.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc2.weight.data.uniform_(-0.1, 0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.tanh(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(network, epsilon, nA):\n",
    "    def policy(state):\n",
    "        sample = random.random()\n",
    "        if sample < (1-epsilon) + (epsilon/nA):\n",
    "            q_values = network(state.view(1, -1))\n",
    "            action = q_values.data.max(1)[1][0, 0]\n",
    "        else:\n",
    "            action = random.randrange(nA)\n",
    "        return action\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.memory = deque()\n",
    "        self.capacity = capacity\n",
    "        \n",
    "    def push(self, transition):\n",
    "        if len(self.memory) > self.capacity:\n",
    "            self.memory.popleft()\n",
    "        self.memory.append(transition)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_tensor(ndarray, volatile=False):\n",
    "    return Variable(torch.from_numpy(ndarray), volatile=volatile).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deep_q_learning(num_episodes=10, batch_size=100,\n",
    "                    discount_factor=0.95, epsilon=0.1, epsilon_decay=0.95):\n",
    "    # Qnetwork and memory\n",
    "    net = Net()\n",
    "    memory = ReplayMemory(10000)\n",
    "    \n",
    "    # Loss and Optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        policy = make_epsilon_greedy_policy(\n",
    "            net, epsilon, env.action_space.n)\n",
    "        \n",
    "        # Start an episode\n",
    "        state = env.reset()\n",
    "        \n",
    "        for t in range(10000):\n",
    "            # Sample action from epsilon greed policy\n",
    "            action = policy(to_tensor(state))\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Restore transition in memory\n",
    "            memory.push([state, action, reward, next_state])\n",
    "            \n",
    "            if len(memory) >= batch_size:\n",
    "                # Sample mini-batch transitions from memory\n",
    "                batch = memory.sample(batch_size)\n",
    "                state_batch = np.vstack([trans[0] for trans in batch])\n",
    "                action_batch = np.vstack([trans[1] for trans in batch])\n",
    "                reward_batch = np.vstack([trans[2] for trans in batch])\n",
    "                next_state_batch = np.vstack([trans[3] for trans in batch])\n",
    "                \n",
    "                # Forward + Backward + Optimize\n",
    "                net.zero_grad()\n",
    "                q_values = net(to_tensor(state_batch))\n",
    "                next_q_values = net(to_tensor(next_state_batch, volatile=True))\n",
    "                next_q_values.volatile = False\n",
    "                \n",
    "                td_target = to_tensor(reward_batch) + discount_factor * (next_q_values).max(1)[0]\n",
    "                loss = criterion(q_values.gather(1,\n",
    "                            to_tensor(action_batch).long().view(-1, 1)), td_target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            if done: \n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "        \n",
    "        if len(memory) >= batch_size and (i_episode+1) % 10 == 0:\n",
    "            print ('episode: %d, time: %d, loss: %.4f' %(i_episode, t, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
